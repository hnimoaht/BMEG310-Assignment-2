---
title: "Assignment2-SL"
author: "Group"
date: "2025-10-15"
output: pdf_document
---

```{r setup, include=FALSE}
    knitr::opts_chunk$set(echo = TRUE)
    library(ggplot2)
    library(reshape2)
```

## Load Data

```{r}
    # load data
    ovarian.dataset <- read.csv("ovarian.csv", sep = ",", header = FALSE)

    # assign column names
    features <- c("perimeter", "area", "smoothness", "symmetry", "concavity", paste("protein", seq(1,25), sep=""))
    colnames(ovarian.dataset) <- c("cell_id", "diagnosis", features)
```

# 1. Dimensionality reduction

## 1.1 Perform PCA on the features of the dataset. How much of the variation in the data is associated with PC1?
```{r}

    #we will be running PCA on the feature columns only, to caputure the variance in the data.

    # removing non-feature columns
    feature.data <- ovarian.dataset[, features]

    # do PCA (center & scale = TRUE)
    pca.result <- prcomp(feature.data, center = TRUE, scale. = TRUE)
    pca.data <- as.data.frame(pca.result$x)
    pca.data$diagnosis <- ovarian.dataset$diagnosis

    # plot PCA
    ggplot(pca.data, aes(x = PC1, y = PC2, color = diagnosis)) + geom_point(size = 2, alpha = 0.7) +
        labs(
            title = "PCA of Ovarian Cell Features",
            x = "Principal Component 1",
            y = "Principal Component 2",
            color = "Diagnosis"
        ) + theme_minimal()

    # view summary of PCA
    summary(pca.result)

    # value of variation associated with PC1
    pc1_variance <- summary(pca.result)$importance[2, 1]
    pc1_variance

```

## 1.2 You want to represent 90% of the variance in the data by dimensionality reduction. How many PCs do you need to achieve this? In other word, what would be the dimensionality of the reduced feature space so that you preserve 90% of the variability in the data?
```{r}

    # get cumulative proportion of variance
    var.explained <- summary(pca.result)$importance[2, ] * 100
    cum.var <- summary(pca.result)$importance[3, ] * 100

    # no. of PCs needed for 90%
    num.pc.90 <- which(cum.var >= 90)[1]
    num.pc.90
    cum.var[num.pc.90]

    # using a plot to visualize the cumulative variance explained
    plot(cum.var, type = "b", pch = 19, 
        xlab = "Number of Principal Components", 
        ylab = "Cumulative Variance Explained (%)",
        main = "Scree Plot: Cumulative Variance Explained")
    abline(h = 90, col = "red", lty = 2)

```


## 1.3 As you should know by now, PCA transforms the data into a new space. In a 2-D plot, can you plot the observations corresponding to the first two important PCs? Note, use two different colors to represent the two classes of cells.
```{r}
    # PCA data frame with diagnosis label
    pca.data <- as.data.frame(pca.result$x)
    pca.data$diagnosis <- ovarian.dataset$diagnosis

    # variance percentages for labeling
    var.explained <- summary(pca.results)$importance[2, 1:2] * 100

    # making plot
    ggplot(pca.data, aes(x = PC1, y = PC2, color = diagnosis)) + geom_point(size = 2, alpha = 0.7) +
    labs(
        title = "PCA Plot of Ovarian Cell Features",
        x = paste0("PC1 (", round(var.explained[1], 1), "% variance)"),
        y = paste0("PC2 (", round(var.explained[2], 1), "% variance)"),
        color = "Diagnosis"
    ) + theme_minimal()
```

## 1.4  Can you plot the "area" and "concavity"  features associated with the cells?
```{r}
    ggplot(ovarian.dataset, aes(x = area, y = concavity, color = diagnosis)) + geom_point(size = 2, alpha = 0.7) +
    labs(
        title = "Area vs Concavity of Ovarian Cell Features",
        x = "Area",
        y = "Concavity",
        color = "Diagnosis"
    ) + theme_minimal()
```

## 1.5 What is the difference between the two plots? Which one gives you better separation between the classes and why?
```{r}
    # The PCA plot provides better separation between the classes compared to the area vs concavity plot.
    # This is because PCA combines information from all features to create new axes (principal components) that maximize variance.
    # In contrast, the area vs concavity plot only considers two features, which may not capture the overall structure of the data as effectively. 
    # The PCA plot thus reveals clearer clustering of the two classes (benign vs malignant) by leveraging the combined variance from all features.
    # Each PC captures the strongest patterns in the data, leading to improved class separation.
```

## 1.6 Plot the distribution of the PCs. Hint: you can use boxplot on the transformed dataset.
```{r}
    # convert PCA data to long format for plotting
    pca.long <- reshape2::melt(pca.data, id.vars = "diagnosis")

    # boxplot of each PC by diagnosis
    ggplot(pca.long, aes(x = variable, y = value, fill = diagnosis)) + geom_boxplot(outlier.size = 0.5, alpha = 0.7) +
    labs(
        title = "Distribution of Principal Components by Diagnosis",
        x = "Principal Component",
        y = "PC Value"
    ) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

    # From the boxplots, we can observe that certain PCs show more distinct separation between the benign and malignant classes.
    # This indicates that these PCs capture features that are more relevant for distinguishing between the two classes.
    # This confirms that the most diagnostic information is contained within the first few principal components.
```

# 2. Clustering

## 2.1 Apply kmeans clustering on the data and identify two clusters within your dataset. What is the concordance between the clusters that you have identified and the true labels of the cells (Benign vs Malignant).

```{r}
    set.seed(123)  # for reproducibility
    feature.data <- scale(ovarian.dataset[, 3:ncol(ovarian.dataset) - 1])  
    # this doesn't include cell_id and diagnosis columns

    # perform kmeans clustering with 2 clusters
    kmeans.result <- kmeans(feature.data, centers = 2, nstart = 25)

    # add cluster assignments to original data
    ovarian.dataset$cluster <- as.factor(kmeans.result$cluster)

    # create a confusion matrix to compare clusters with true labels
    confusion.matrix <- table(Cluster = ovarian.dataset$cluster, Diagnosis = ovarian.dataset$diagnosis)
    confusion.matrix

    # calculate accuracy
    accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
    accuracy
```

## 2.2 Repeat the kmeans analysis 10 times and report the mean accuracy across the 10 runs. Why are the results different in each run? 

```{r}
    set.seed(123)  # for reproducibility
    accuracies <- numeric(10)

    # loop to run kmeans 10 times
    for (i in 1:10) {
        kmeans.result <- kmeans(feature.data, centers = 2, nstart = 25)
        ovarian.dataset$cluster <- as.factor(kmeans.result$cluster)
        confusion.matrix <- table(Cluster = ovarian.dataset$cluster, Diagnosis = ovarian.dataset$diagnosis)
        accuracies[i] <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
    }

    # calculate mean accuracy
    mean.accuracy <- mean(accuracies)
    mean.accuracy

    # The results differ in each run because k-means clustering is sensitive to the initial placement of centroids.
    # Different initializations can lead to different cluster assignments, especially in datasets with overlapping clusters or
    # complex structures. Using multiple starts (nstart) helps mitigate this by exploring various initial configurations.
```

## 2.3 Repeat the same analysis but with the top 5 PCs. 
```{r}
    set.seed(123)  # for reproducibility
    pca.feature.data <- pca.data[, 1:5]  # using top 5 PCs
    accuracies.pca <- numeric(10)

    # loop to run kmeans 10 times on PCA data
    for (i in 1:10) {
        kmeans.result <- kmeans(pca.feature.data, centers = 2, nstart = 25)
        pca.data$cluster <- as.factor(kmeans.result$cluster)
        confusion.matrix <- table(Cluster = pca.data$cluster, Diagnosis = pca.data$diagnosis)
        accuracies.pca[i] <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
    }

    # calculate mean accuracy for PCA-based clustering
    mean.accuracy.pca <- mean(accuracies.pca)
    mean.accuracy.pca
```

## 2.4 Compare the results obtained in 2.2 and 2.3.
```{r}
    # The mean accuracy obtained using the original feature set (2.2) is compared to that obtained using the top 5 PCs (2.3).
    # Typically, clustering on PCA-reduced data can yield similar or even improved accuracy due to noise reduction and
    # elimination of redundant features. However, the actual results may vary based on the dataset characteristics.
    # In this case, we can observe whether dimensionality reduction via PCA has enhanced the clustering performance.
```

# 3. Classification
```{r}

    ovarian.dataset.clean <- ovarian.dataset[, !(names(ovarian.dataset) %in% c("cluster"))]

    set.seed(123)  # reproducibility

    # randomize all row indices once
    rows <- sample(nrow(ovarian.dataset.clean))

    # split into two halves
    train.indices <- rows[1:(nrow(ovarian.dataset.clean)/2)]
    test.indices <- rows[(nrow(ovarian.dataset.clean)/2 + 1):nrow(ovarian.dataset.clean)]

    # assign subsets
    ovarian.dataset.train <- ovarian.dataset.clean[train.indices, ]
    ovarian.dataset.test <- ovarian.dataset.clean[test.indices, ]

```

## 3.1 Design a logistic regression classifier to identify (differentiate) benign and malignant cells. Report the performance of the classification technique on the training and test sets. You can report accuracy, precision and recall. Compare the performance of the classifier on the training and test set and provide a reason as to why one is better than the other.
```{r}
    # fit logistic regression model
    logit.model <- glm(diagnosis ~ ., data = ovarian.dataset.train[, -1], family = binomial)

    # make predictions on training set
    train.probs <- predict(logit.model, type = "response")
    train.pred <- ifelse(train.probs > 0.5, "M", "B")

    # confusion matrix for training set
    train.confusion <- table(Predicted = train.pred, Actual = ovarian.dataset.train$diagnosis)

    # calculate accuracy, precision, recall for training set
    train.accuracy <- sum(diag(train.confusion)) / sum(train.confusion)
    train.precision <- train.confusion["M", "M"] / sum(train.confusion["M", ])
    train.recall <- train.confusion["M", "M"] / sum(train.confusion[, "M"])

    # make predictions on test set
    test.probs <- predict(logit.model, newdata = ovarian.dataset.test[, -1], type = "response")
    test.pred <- ifelse(test.probs > 0.5, "M", "B")

    # confusion matrix for test set
    test.confusion <- table(Predicted = test.pred, Actual = ovarian.dataset.test$diagnosis)

    # calculate accuracy, precision, recall for test set
    test.accuracy <- sum(diag(test.confusion)) / sum(test.confusion)
    test.precision <- test.confusion["M", "M"] / sum(test.confusion["M", ])
    test.recall <- test.confusion["M", "M"] / sum(test.confusion[, "M"])

    # output results
    list(
        Train = list(Accuracy = train.accuracy, Precision = train.precision, Recall = train.recall),
        Test = list(Accuracy = test.accuracy, Precision = test.precision, Recall = test.recall)
    )
```

## 3.2 Repeat the same task as Q3.1. with the top 5 PCs.


## 3.3 Compare the results between Q3.1. and Q3.2. Do the results get better or worse? Why?

## 3.4 Compare the results of the clustering and classification methods. Which one gives you better result?

## 3.5 Install the library "ROCR" and load it. Then, run the following lines using your trained logistic regression model:

## 3.6 Design another classifier (using a different classification method) and repeat Q3.1-3.





